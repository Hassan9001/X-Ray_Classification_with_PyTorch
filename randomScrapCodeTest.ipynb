{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae129b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ORIGINAL ####\n",
    "# -------\n",
    "# Install\n",
    "# -------\n",
    "# !pip install torch torchvision torchmetrics\n",
    "\n",
    "# -------------------------\n",
    "# Import required libraries\n",
    "# -------------------------\n",
    "# Data loading\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "# Train model\n",
    "import torch\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Evaluate model\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "# Check for GPU availability\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(\"Using device:\", device)\n",
    "# Check for MPS availability\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "Using device: mps\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "# Move 50 random images per class from the training set to create a validation set\n",
    "#---------------------------------------------------------------------------------\n",
    "def move_files(src_class_dir, dest_class_dir, n=50):\n",
    "    if not os.path.exists(dest_class_dir):\n",
    "        os.makedirs(dest_class_dir)\n",
    "    files = os.listdir(src_class_dir)\n",
    "    random_files = random.sample(files, n)\n",
    "    for f in random_files:\n",
    "        shutil.move(os.path.join(src_class_dir, f), os.path.join(dest_class_dir, f))\n",
    "if not os.path.exists('data/chestxrays/val'):\n",
    "    move_files('data/chestxrays/train/NORMAL', 'data/chestxrays/val/NORMAL')\n",
    "    move_files('data/chestxrays/train/PNEUMONIA', 'data/chestxrays/val/PNEUMONIA')\n",
    "\n",
    "\n",
    "#------------------------------------\n",
    "# Transformations and create datasets\n",
    "#------------------------------------\n",
    "# Define the transformations to apply to the images for use with ResNet-50.\n",
    "# The images need to be normalized to the same domain as the original training data of ResNet-50 network.\n",
    "# Normalize the X-rays using transforms. A Normalize function that takes as input the means and\n",
    "# standard deviations of the three color channels, (R,G,B), from the original ResNet-50 training dataset.\n",
    "transform_mean = [0.485, 0.456, 0.406]\n",
    "transform_std = [0.229, 0.224, 0.225]\n",
    "# Training transforms: Add horizontal flip for augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=transform_mean, std=transform_std)\n",
    "])\n",
    "# Validation and test transforms: no augmentation, just normalization\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=transform_mean, std=transform_std)\n",
    "])\n",
    "# Create datasets\n",
    "train_dataset = ImageFolder('data/chestxrays/train', transform=train_transform)\n",
    "val_dataset = ImageFolder('data/chestxrays/val', transform=val_test_transform)\n",
    "test_dataset = ImageFolder('data/chestxrays/test', transform=val_test_transform)\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "print(\"Training set size:\", len(train_dataset))\n",
    "print(\"Validation set size:\", len(val_dataset))\n",
    "print(\"Test set size:\", len(test_dataset))\n",
    "\n",
    "\n",
    "#----------------------\n",
    "# Instantiate the model\n",
    "#----------------------\n",
    "# Load the pre-trained ResNet-50 model with new weights with accuracy 80.858%\n",
    "resnet50 = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "#-----------------\n",
    "# Modify the model\n",
    "#-----------------\n",
    "# Freeze the parameters of the model\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "# Modify the final layer for binary classification\n",
    "resnet50.fc = nn.Linear(resnet50.fc.in_features, 1)\n",
    "# Set the model to ResNet-50\n",
    "model = resnet50\n",
    "# Move the model to the selected device (GPU, MPS, or CPU)\n",
    "model.to(device)\n",
    "#-------------------------\n",
    "# Define the training loop\n",
    "#-------------------------\n",
    "# Training function with validation and early stopping\n",
    "def train_with_validation(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=50, patience=5):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0\n",
    "for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "            \n",
    "            # Use mixed precision training for forward pass and loss computation\n",
    "            with torch.amp.autocast(device_type=device.type):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "preds = torch.sigmoid(outputs) > 0.5\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_accuracy += torch.sum(preds == labels.data)\n",
    "train_loss = running_loss / len(train_loader.dataset)\n",
    "        # train_acc = running_accuracy.double() / len(train_loader.dataset) # For GPU/CPU\n",
    "        train_acc = running_accuracy.float() / len(train_loader.dataset) # For MPS\n",
    "# Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                labels = labels.float().unsqueeze(1)\n",
    "                \n",
    "                # Use mixed precision training for forward pass and loss computation\n",
    "                with torch.amp.autocast(device_type=device.type):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_accuracy += torch.sum(preds == labels.data)\n",
    "val_loss = val_loss / len(val_loader.dataset)\n",
    "        # val_acc = val_accuracy.double() / len(val_loader.dataset)  # For GPU/CPU\n",
    "        val_acc = val_accuracy.float() / len(val_loader.dataset) # For MPS\n",
    "print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "# Step the scheduler with the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "# Early Stopping Check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save model\n",
    "            torch.save(model.state_dict(), 'model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "print(\"Training complete. Best validation loss: {:.4f}\".format(best_val_loss))\n",
    "\n",
    "\n",
    "#--------------------\n",
    "# Fine-tune the model\n",
    "#--------------------\n",
    "# Set up loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "# Use ReduceLROnPlateau scheduler to reduce LR if validation loss stagnates\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "# Decay lr by 10% every epoch (alternative scheduler)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "# Train the model with early stopping and validation\n",
    "train_with_validation(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=50, patience=5)\n",
    "\n",
    "\n",
    "\n",
    "#-------------------\n",
    "# Evaluate the model\n",
    "#-------------------\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "# Load the best model weights\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "# Initialize metrics for accuracy and F1 score\n",
    "accuracy_metric = Accuracy(task=\"binary\")\n",
    "f1_metric = F1Score(task=\"binary\")\n",
    "# Create lists store all predictions and labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "# Disable gradient calculation for evaluation\n",
    "with torch.no_grad():\n",
    "  for inputs, labels in test_loader:\n",
    "    # Move inputs and labels to the device\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    preds = torch.sigmoid(outputs).round() # Round to 0 or 1\n",
    "    \n",
    "    # Extend the lists with predictions and labels\n",
    "    all_preds.extend(preds.cpu().tolist())\n",
    "    all_labels.extend(labels.unsqueeze(1).cpu().tolist())\n",
    "# Convert lists to tensors\n",
    "all_preds = torch.tensor(all_preds)\n",
    "all_labels = torch.tensor(all_labels)\n",
    "# Compute metrics for the entire test set\n",
    "test_acc = accuracy_metric(all_preds, all_labels).item()\n",
    "test_f1 = f1_metric(all_preds, all_labels).item()\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")\n",
    "print(f\"Test F1-score: {test_f1:.3f}\")\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model_final.pth')\n",
    "# Save the model with TorchScript for deployment\n",
    "model_scripted = torch.jit.script(model)  # Scripting the model\n",
    "torch.jit.save(model_scripted, 'model_scripted.pt')\n",
    "# Save the model with TorchScript for deployment\n",
    "model_traced = torch.jit.trace(model, torch.randn(1, 3, 224, 224).to(device))  # Tracing the model\n",
    "torch.jit.save(model_traced, 'model_traced.pt')\n",
    "# Load the model for inference\n",
    "# Load the model with TorchScript for deployment\n",
    "# model_loaded = torch.jit.load('model_scripted.pt')\n",
    "# model_loaded = torch.jit.load('model_traced.pt')\n",
    "# model_loaded.eval()\n",
    "# # Example inference with the loaded model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a2c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COPPILOT ###\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Improved training script for binary classification on chest X-ray images.\n",
    "This script:\n",
    "  - Creates a validation set by moving random files\n",
    "  - Applies proper transforms (and optional augmentation)\n",
    "  - Loads a pre-trained ResNet-50 and fine-tunes its final layer\n",
    "  - Trains using early stopping with a validation loop and learning rate scheduler\n",
    "  - Evaluates the model on a test set with accuracy and F1-score metrics\n",
    "  - Saves the trained model in both state dict and TorchScript formats\n",
    "\"\"\"\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "# from tqdm import tqdm  # Optional: progress bar for loops\n",
    "from tqdm.notebook import tqdm  # Notebook-friendly progress bar\n",
    "\n",
    "# ---------------------------\n",
    "# Set random seeds for reproducibility\n",
    "# ---------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# ---------------------------\n",
    "# Device selection\n",
    "# ---------------------------\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Utility Function: Move files for validation set creation\n",
    "# ---------------------------\n",
    "def move_files(src_class_dir: str, dest_class_dir: str, n: int = 50) -> None:\n",
    "    \"\"\"\n",
    "    Moves n random files from the source directory to the destination directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dest_class_dir):\n",
    "        os.makedirs(dest_class_dir)\n",
    "    files = os.listdir(src_class_dir)\n",
    "    if len(files) < n:\n",
    "        raise ValueError(f\"Not enough files in {src_class_dir} to move {n} files.\")\n",
    "    random_files = random.sample(files, n)\n",
    "    for f in random_files:\n",
    "        shutil.move(os.path.join(src_class_dir, f), os.path.join(dest_class_dir, f))\n",
    "# ---------------------------\n",
    "# Data preparation: Create validation set if needed\n",
    "# ---------------------------\n",
    "def prepare_validation_set():\n",
    "    if not os.path.exists('data/chestxrays/val'):\n",
    "        move_files('data/chestxrays/train/NORMAL', 'data/chestxrays/val/NORMAL', n=50)\n",
    "        move_files('data/chestxrays/train/PNEUMONIA', 'data/chestxrays/val/PNEUMONIA', n=50)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Data transforms and datasets\n",
    "# ---------------------------\n",
    "def prepare_datasets():\n",
    "    \"\"\"\n",
    "    Defines image transformations and creates training, validation, and test datasets.\n",
    "    \"\"\"\n",
    "    # ResNet-50 expects images normalized using its original channel means and stds.\n",
    "    transform_mean = [0.485, 0.456, 0.406]\n",
    "    transform_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    # Define augmentation for training and simple normalization for validation and testing\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=transform_mean, std=transform_std)\n",
    "    ])\n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=transform_mean, std=transform_std)\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder('data/chestxrays/train', transform=train_transform)\n",
    "    val_dataset = datasets.ImageFolder('data/chestxrays/val', transform=val_test_transform)\n",
    "    test_dataset = datasets.ImageFolder('data/chestxrays/test', transform=val_test_transform)\n",
    "\n",
    "    print(\"Training set size:\", len(train_dataset))\n",
    "    print(\"Validation set size:\", len(val_dataset))\n",
    "    print(\"Test set size:\", len(test_dataset))\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Build DataLoaders\n",
    "# ---------------------------\n",
    "def prepare_dataloaders(train_dataset, val_dataset, test_dataset, batch_size: int = 32):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained ResNet-50 model, freezes its parameters,\n",
    "    and replaces the final fully connected layer for binary classification.\n",
    "    \"\"\"\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Model setup: Load and modify ResNet-50 for binary classification\n",
    "# ---------------------------\n",
    "def build_model() -> torch.nn.Module:\n",
    "    model_resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "    # Freeze all parameters\n",
    "    for param in model_resnet.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace the final classification layer with one that outputs a single logit (for binary classification)\n",
    "    num_features = model_resnet.fc.in_features\n",
    "    model_resnet.fc = nn.Linear(num_features, 1)\n",
    "    model_resnet = model_resnet.to(device)\n",
    "    return model_resnet\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Training loop with validation and early stopping\n",
    "# ---------------------------\n",
    "def train_with_validation(model: torch.nn.Module,\n",
    "                          train_loader: DataLoader,\n",
    "                          val_loader: DataLoader,\n",
    "                          criterion,\n",
    "                          optimizer,\n",
    "                          scheduler,\n",
    "                          device: torch.device,\n",
    "                          num_epochs: int = 50,\n",
    "                          patience: int = 5) -> None:\n",
    "            patience: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Trains the model with a training loop that includes validation after each epoch. \n",
    "    Saves the model if validation loss improves.\n",
    "    Early stops training if the validation loss does not improve for 'patience' epochs.\n",
    "    \"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Training phase\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Adjust labels shape: add extra dimension and cast to float\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.float() / len(train_loader.dataset)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                labels = labels.float().unsqueeze(1)\n",
    "                with torch.amp.autocast(device_type=device.type):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = val_corrects.float() / len(val_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Step LR scheduler based on validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    print(\"Training complete. Best validation loss: {:.4f}\".format(best_val_loss))\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Model evaluation on test set\n",
    "# ---------------------------\n",
    "def evaluate_model(model: torch.nn.Module, test_loader: DataLoader):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and prints Accuracy and F1-score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    accuracy_metric = Accuracy(task=\"binary\")\n",
    "    f1_metric = F1Score(task=\"binary\")\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.sigmoid(outputs).round()  # Binary prediction: round to 0 or 1\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "            # Adjust labels to a comparable shape\n",
    "            all_labels.extend(labels.unsqueeze(1).cpu().numpy().tolist())\n",
    "\n",
    "    # Convert lists to tensors to compute final metrics\n",
    "    all_preds_tensor = torch.tensor(all_preds)\n",
    "    all_labels_tensor = torch.tensor(all_labels)\n",
    "\n",
    "    test_acc = accuracy_metric(all_preds_tensor, all_labels_tensor).item()\n",
    "    test_f1 = f1_metric(all_preds_tensor, all_labels_tensor).item()\n",
    "\n",
    "    print(f\"Test accuracy: {test_acc:.3f}\")\n",
    "    print(f\"Test F1-score: {test_f1:.3f}\")\n",
    "    return test_acc, test_f1\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Save model in different formats for deployment\n",
    "# ---------------------------\n",
    "def save_model_formats(model: torch.nn.Module, device: torch.device):\n",
    "    \"\"\"\n",
    "    Saves the final model state as a state dict, as well as in TorchScript's scripted and traced formats.\n",
    "    \"\"\"\n",
    "    # Save the final state dict\n",
    "    torch.save(model.state_dict(), 'model_final.pth')\n",
    "    # Save a scripted model\n",
    "    model_scripted = torch.jit.script(model)\n",
    "    torch.jit.save(model_scripted, 'model_scripted.pt')\n",
    "    # Save a traced model using a dummy input (assuming images are 224x224 with 3 channels)\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "    model_traced = torch.jit.trace(model, dummy_input)\n",
    "    torch.jit.save(model_traced, 'model_traced.pt')\n",
    "    print(\"Model saved in state dict, scripted, and traced formats.\")\n",
    "\n",
    "\n",
    "# # ---------------------------\n",
    "# # Main routine: Prepare data, build model, train, evaluate, and save.\n",
    "# # ---------------------------\n",
    "# def main():\n",
    "#     prepare_validation_set()\n",
    "#     train_dataset, val_dataset, test_dataset = prepare_datasets()\n",
    "#     train_loader, val_loader, test_loader = prepare_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=32)\n",
    "#     model = build_model()\n",
    "\n",
    "#     # Set up loss, optimizer, and LR scheduler\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "#     optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "#     train_with_validation(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=50, patience=5)\n",
    "\n",
    "#     # Load the best model based on validation loss and evaluate on the test set\n",
    "#     model.load_state_dict(torch.load('model.pth'))\n",
    "#     evaluate_model(model, test_loader)\n",
    "\n",
    "#     # Save model formats for deployment\n",
    "#     save_model_formats(model, device)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() \n",
    "\n",
    "\n",
    "### Main Execution Pipeline ###\n",
    "\n",
    "# Prepare the validation set if needed\n",
    "prepare_validation_set()\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset, val_dataset, test_dataset = prepare_datasets()\n",
    "train_loader, val_loader, test_loader = prepare_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=32)\n",
    "# Build the model for binary classification\n",
    "model = build_model()\n",
    "\n",
    "# Set up loss, optimizer, and learning rate scheduler\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "# Train the model with validation and early stopping\n",
    "train_with_validation(model, train_loader, val_loader, criterion, optimizer, scheduler, device,\n",
    "                      num_epochs=50, patience=5)\n",
    "\n",
    "# Load the best model based on validation loss\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "# Evaluate the model on the test set\n",
    "evaluate_model(model, test_loader)\n",
    "\n",
    "# Save the model in various formats for later deployment\n",
    "save_model_formats(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e313895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dc467a-5830-44c0-ab74-435be0e5593c",
   "metadata": {},
   "source": [
    "Pneumonia is one of the leading respiratory illnesses worldwide, and its timely and accurate diagnosis is crucial for effective treatment. Manually reviewing chest X-rays plays a critical role in this process, but AI can significantly expedite and enhance assessments.\n",
    "\n",
    "In this project, I explored the ability of a deep learning model to distinguish pneumonia cases from normal lung X-ray images. I fine-tuned a pre-trained ResNet-18 convolutional neural network to classify X-rays into two categories: normal lungs and those affected by pneumonia. Leveraging the pre-trained weights of ResNet-18 allowed me to create an accurate classifier efficiently, reducing the resources and time needed for training.\n",
    "\n",
    "## The Data\n",
    "\n",
    "<img src=\"x-rays_sample.png\" align=\"center\"/>\n",
    "&nbsp\n",
    "\n",
    "The dataset consisted of 300 chest X-rays for training and 100 for testing, evenly divided between NORMAL and PNEUMONIA categories. The images had been preprocessed and organized into train and test folders, with data loaders ready for use with PyTorch. This project highlights my ability to implement advanced deep learning techniques in a healthcare context, showcasing how AI improves diagnostic processes.ses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0f522b79-2a5a-4472-adb9-0d924870bfa1",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 3074,
    "lastExecutedAt": 1733403974567,
    "lastExecutedByKernel": "13046523-b15f-47cc-8175-b2751ab4a4e6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# # Make sure to run this cell to use torchmetrics.\n!pip install torch torchvision torchmetrics",
    "outputsMetadata": {
     "0": {
      "height": 514,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.0)\n",
      "Requirement already satisfied: torchmetrics in /home/repl/.local/lib/python3.8/site-packages (1.5.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.6.3)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.23.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (23.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /home/repl/.local/lib/python3.8/site-packages (from torchmetrics) (0.11.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torchvision) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision) (2019.11.28)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# # Make sure to run this cell to use torchmetrics.\n",
    "!pip install torch torchvision torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cb1bedee-bcd5-4c80-a5ed-93df89af0295",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 54,
    "lastExecutedAt": 1733403974623,
    "lastExecutedByKernel": "13046523-b15f-47cc-8175-b2751ab4a4e6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import required libraries\n# Data loading\nimport random\nimport numpy as np\nfrom torchvision.transforms import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\n\n# Train model\nimport torch\nfrom torchvision import models\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Evaluate model\nfrom torchmetrics import Accuracy, F1Score\n\n# Set random seeds for reproducibility\ntorch.manual_seed(101010)\nnp.random.seed(101010)\nrandom.seed(101010)"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "# Data loading\n",
    "import random\n",
    "import numpy as np\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Train model\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluate model\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(101010)\n",
    "np.random.seed(101010)\n",
    "random.seed(101010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dd91680d-cb63-4876-9a51-4ee6bb250c7d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1733403974672,
    "lastExecutedByKernel": "13046523-b15f-47cc-8175-b2751ab4a4e6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import os\nimport zipfile\n\n# Unzip the data folder\nif not os.path.exists('data/chestxrays'):\n    with zipfile.ZipFile('data/chestxrays.zip', 'r') as zip_ref:\n        zip_ref.extractall('data')"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Unzip the data folder\n",
    "if not os.path.exists('data/chestxrays'):\n",
    "    with zipfile.ZipFile('data/chestxrays.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0cc5591a-8dc1-4d7f-88d2-3b1a59fb2a5f",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1733403974719,
    "lastExecutedByKernel": "13046523-b15f-47cc-8175-b2751ab4a4e6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# The images need to be normalized to the same domain as the original training data of ResNet-18 network.\n# Normalize the X-rays using transforms.\n# standard deviations of the three color channels, (R,G,B), from the original ResNet-18 training dataset.\ntransform_mean = [0.485, 0.456, 0.406]\ntransform_std =[0.229, 0.224, 0.225]\ntransform = transforms.Compose([transforms.ToTensor(), \n                                transforms.Normalize(mean=transform_mean, std=transform_std)])\n\n# Apply the image transforms\ntrain_dataset = ImageFolder('data/chestxrays/train', transform=transform)\ntest_dataset = ImageFolder('data/chestxrays/test', transform=transform)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=len(train_dataset) // 2, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=len(test_dataset))"
   },
   "outputs": [],
   "source": [
    "# Normalize the X-rays using transforms.\n",
    "# standard deviations of the three color channels, (R,G,B), from the original ResNet-18 training dataset.\n",
    "transform_mean = [0.485, 0.456, 0.406]\n",
    "transform_std =[0.229, 0.224, 0.225]\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize(mean=transform_mean, std=transform_std)])\n",
    "\n",
    "# Apply the image transforms\n",
    "train_dataset = ImageFolder('data/chestxrays/train', transform=transform)\n",
    "test_dataset = ImageFolder('data/chestxrays/test', transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset) // 2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c99cf95b-83f3-49e4-9777-4e70736452d8",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 186,
    "lastExecutedAt": 1733403974905,
    "lastExecutedByKernel": "13046523-b15f-47cc-8175-b2751ab4a4e6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Instantiate the model\n# Load the pre-trained ResNet-18 model\nresnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "# Load the pre-trained ResNet-18 model\n",
    "resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a0d3bc0c-c779-4c24-a7ec-c2a2c9a46549",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 53,
    "lastExecutedAt": 1733403974959,
    "lastExecutedByKernel": "13046523-b15f-47cc-8175-b2751ab4a4e6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Modify the model\n# Freeze the parameters of the model\nfor param in resnet18.parameters():\n    param.requires_grad = False\n\n# Modify the final layer for binary classification\nresnet18.fc = nn.Linear(resnet18.fc.in_features, 1)"
   },
   "outputs": [],
   "source": [
    "# Modify the model\n",
    "# Freeze the parameters of the model\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the final layer for binary classification\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "99d9bd21-2cbb-4056-a187-48c171af798b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 56,
    "lastExecutedAt": 1733403975015,
    "lastExecutedByKernel": "13046523-b15f-47cc-8175-b2751ab4a4e6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Define the training loop\n# Model training/fine-tuning loop\ndef train(model, train_loader, criterion, optimizer, num_epochs):\n    \n    # Train the model for the specified number of epochs\n    for epoch in range(num_epochs):\n        # Set the model to train mode\n        model.train()\n\n        # Initialize the running loss and accuracy\n        running_loss = 0.0\n        running_accuracy = 0\n\n        # Iterate over the batches of the train loader\n        for inputs, labels in train_loader:\n\n            # Zero the optimizer gradients\n            optimizer.zero_grad()\n            \n            # Ensure labels have the same dimensions as outputs\n            labels = labels.float().unsqueeze(1)\n\n            # Forward pass\n            outputs = model(inputs)\n            preds = torch.sigmoid(outputs) > 0.5 # Binary classification\n            loss = criterion(outputs, labels)\n\n            # Backward pass and optimizer step\n            loss.backward()\n            optimizer.step()\n\n            # Update the running loss and accuracy\n            running_loss += loss.item() * inputs.size(0)\n            running_accuracy += torch.sum(preds == labels.data)\n\n        # Calculate the train loss and accuracy for the current epoch\n        train_loss = running_loss / len(train_dataset)\n        train_acc = running_accuracy.double() / len(train_dataset)\n\n        # Print the epoch results\n        print('Epoch [{}/{}], train loss: {:.4f}, train acc: {:.4f}'\n              .format(epoch+1, num_epochs, train_loss, train_acc))\n"
   },
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "# Model training/fine-tuning loop\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    \n",
    "    # Train the model for the specified number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to train mode\n",
    "        model.train()\n",
    "\n",
    "        # Initialize the running loss and accuracy\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0\n",
    "\n",
    "        # Iterate over the batches of the train loader\n",
    "        for inputs, labels in train_loader:\n",
    "\n",
    "            # Zero the optimizer gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Ensure labels have the same dimensions as outputs\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.sigmoid(outputs) > 0.5 # Binary classification\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimizer step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the running loss and accuracy\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_accuracy += torch.sum(preds == labels.data)\n",
    "\n",
    "        # Calculate the train loss and accuracy for the current epoch\n",
    "        train_loss = running_loss / len(train_dataset)\n",
    "        train_acc = running_accuracy.double() / len(train_dataset)\n",
    "\n",
    "        # Print the epoch results\n",
    "        print('Epoch [{}/{}], train loss: {:.4f}, train acc: {:.4f}'\n",
    "              .format(epoch+1, num_epochs, train_loss, train_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e51a7e2e-5e0f-4f17-b2ff-78004551390c",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 71121,
    "lastExecutedAt": 1733404046136,
    "lastExecutedByKernel": "13046523-b15f-47cc-8175-b2751ab4a4e6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#Fine-tune the model       \n# Set the model to ResNet-18\nmodel = resnet18\n\n# Fine-tune the ResNet-18 model for 3 epochs using the train_loader\noptimizer = torch.optim.Adam(model.fc.parameters(), lr=0.01)\ncriterion = torch.nn.BCEWithLogitsLoss()\ntrain(model, train_loader, criterion, optimizer, num_epochs=3)",
    "outputsMetadata": {
     "0": {
      "height": 80,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], train loss: 1.3915, train acc: 0.4567\n",
      "Epoch [2/3], train loss: 0.8973, train acc: 0.4633\n",
      "Epoch [3/3], train loss: 0.9199, train acc: 0.5033\n"
     ]
    }
   ],
   "source": [
    "#Fine-tune the model       \n",
    "# Set the model to ResNet-18\n",
    "model = resnet18\n",
    "\n",
    "# Fine-tune the ResNet-18 model for 3 epochs using the train_loader\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "train(model, train_loader, criterion, optimizer, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7e0e1ad6-2f78-4a14-943b-8cc7c9dfe960",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 7952,
    "lastExecutedAt": 1733404054089,
    "lastExecutedByKernel": "13046523-b15f-47cc-8175-b2751ab4a4e6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Evaluate the model\n\n# Set model to evaluation mode\nmodel = resnet18\nmodel.eval()\n\n# Initialize metrics for accuracy and F1 score\naccuracy_metric = Accuracy(task=\"binary\")\nf1_metric = F1Score(task=\"binary\")\n\n# Create lists store all predictions and labels\nall_preds = []\nall_labels = []\n\n# Disable gradient calculation for evaluation\nwith torch.no_grad():\n  for inputs, labels in test_loader:\n    # Forward pass\n    outputs = model(inputs)\n    preds = torch.sigmoid(outputs).round()  # Round to 0 or 1\n\n    # Extend the lists with predictions and labels\n    all_preds.extend(preds.tolist())\n    all_labels.extend(labels.unsqueeze(1).tolist())\n\n  # Convert lists back to tensors\n  all_preds = torch.tensor(all_preds)\n  all_labels = torch.tensor(all_labels)\n\n  # Calculate accuracy and F1 score\n  test_accuracy = accuracy_metric(all_preds, all_labels).item()\n  test_f1_score = f1_metric(all_preds, all_labels).item()\n  print(f\"\\nTest accuracy: {test_acc:.3f}\\nTest F1-score: {test_f1:.3f}\")",
    "outputsMetadata": {
     "0": {
      "height": 80,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.580\n",
      "Test F1-score: 0.704\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model = resnet18\n",
    "model.eval()\n",
    "\n",
    "# Initialize metrics for accuracy and F1 score\n",
    "accuracy_metric = Accuracy(task=\"binary\")\n",
    "f1_metric = F1Score(task=\"binary\")\n",
    "\n",
    "# Create lists store all predictions and labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Disable gradient calculation for evaluation\n",
    "with torch.no_grad():\n",
    "  for inputs, labels in test_loader:\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    preds = torch.sigmoid(outputs).round()  # Round to 0 or 1\n",
    "\n",
    "    # Extend the lists with predictions and labels\n",
    "    all_preds.extend(preds.tolist())\n",
    "    all_labels.extend(labels.unsqueeze(1).tolist())\n",
    "\n",
    "  # Convert lists back to tensors\n",
    "  all_preds = torch.tensor(all_preds)\n",
    "  all_labels = torch.tensor(all_labels)\n",
    "\n",
    "  # Calculate accuracy and F1 score\n",
    "  test_accuracy = accuracy_metric(all_preds, all_labels).item()\n",
    "  test_f1_score = f1_metric(all_preds, all_labels).item()\n",
    "  print(f\"\\nTest accuracy: {test_acc:.3f}\\nTest F1-score: {test_f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
